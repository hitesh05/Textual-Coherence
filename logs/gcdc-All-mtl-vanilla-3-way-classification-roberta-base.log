05/04/2023 04:32:32 AM [INFO] featurizing the gcdc corpus: all for task: 3-way-classification with model architecture: mtl
05/04/2023 04:32:34 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 04:32:34 AM [DEBUG] working on train dataset 
05/04/2023 04:32:34 AM [DEBUG] <Done>
05/04/2023 04:32:34 AM [DEBUG] working on dev dataset 
05/04/2023 04:32:35 AM [DEBUG] <Done>
05/04/2023 04:32:35 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:32:35 AM [INFO] post-processing the dataset
05/04/2023 04:32:35 AM [INFO] data preprocessed: 3200, sentences preprocessed: 3200
05/04/2023 04:32:35 AM [INFO] featurizing the datasets..
05/04/2023 04:32:35 AM [DEBUG] 3200 data instance processed. max sent_seq_length: 512
05/04/2023 04:32:37 AM [INFO] post-processing the dataset
05/04/2023 04:32:37 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 04:32:37 AM [INFO] featurizing the datasets..
05/04/2023 04:32:37 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 04:32:38 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:32:38 AM [INFO] label distribution in train dataset (total count: 3200)
05/04/2023 04:32:38 AM [INFO] {2: 1520, 0: 1047, 1: 633}
05/04/2023 04:32:38 AM [INFO] label distribution in dev dataset (total count: 800)
05/04/2023 04:32:38 AM [INFO] {2: 379, 0: 262, 1: 159}
05/04/2023 04:32:38 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:32:38 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 04:32:40 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 04:32:40 AM [DEBUG] working on train dataset 
05/04/2023 04:32:40 AM [DEBUG] <Done>
05/04/2023 04:32:40 AM [DEBUG] working on dev dataset 
05/04/2023 04:32:40 AM [DEBUG] <Done>
05/04/2023 04:32:40 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:32:41 AM [INFO] post-processing the dataset
05/04/2023 04:32:41 AM [INFO] data preprocessed: 2489, sentences preprocessed: 2489
05/04/2023 04:32:41 AM [INFO] featurizing the datasets..
05/04/2023 04:32:41 AM [DEBUG] 2489 data instance processed. max sent_seq_length: 512
05/04/2023 04:32:41 AM [INFO] post-processing the dataset
05/04/2023 04:32:41 AM [INFO] data preprocessed: 277, sentences preprocessed: 277
05/04/2023 04:32:41 AM [INFO] featurizing the datasets..
05/04/2023 04:32:41 AM [DEBUG] 277 data instance processed. max sent_seq_length: 512
05/04/2023 04:32:45 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:32:45 AM [INFO] label distribution for train textual entailment dataset (total count: 2489)
05/04/2023 04:32:45 AM [INFO] {0: 1240, 1: 1249}
05/04/2023 04:32:45 AM [INFO] label distribution for dev textual entailment dataset (total count: 277)
05/04/2023 04:32:45 AM [INFO] {0: 131, 1: 146}
05/04/2023 04:32:45 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:32:45 AM [INFO] 
command line argument captured ..
05/04/2023 04:32:45 AM [INFO] ------------------------------------------------------------
05/04/2023 04:32:45 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 04:32:45 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints
05/04/2023 04:32:45 AM [INFO] gpus - 1
05/04/2023 04:32:45 AM [INFO] epochs - 2
05/04/2023 04:32:45 AM [INFO] batch_size - 2
05/04/2023 04:32:45 AM [INFO] learning_rate - 1e-06
05/04/2023 04:32:45 AM [INFO] clip_grad_norm - 0.0
05/04/2023 04:32:45 AM [INFO] weight_decay - 0.01
05/04/2023 04:32:45 AM [INFO] dropout_rate - 0.1
05/04/2023 04:32:45 AM [INFO] enable_scheduler - False
05/04/2023 04:32:45 AM [INFO] warmup_steps - 0.01
05/04/2023 04:32:45 AM [INFO] margin - 1.0
05/04/2023 04:32:45 AM [INFO] corpus - gcdc
05/04/2023 04:32:45 AM [INFO] sub_corpus - all
05/04/2023 04:32:45 AM [INFO] max_seq_len - 512
05/04/2023 04:32:45 AM [INFO] max_fact_count - 50
05/04/2023 04:32:45 AM [INFO] max_fact_seq_len - 50
05/04/2023 04:32:45 AM [INFO] permutation_count - 20
05/04/2023 04:32:45 AM [INFO] with_replacement - 1
05/04/2023 04:32:45 AM [INFO] train_dataset_count - 5689
05/04/2023 04:32:45 AM [INFO] val_dataset_count - 1077
05/04/2023 04:32:45 AM [INFO] test_dataset_count - None
05/04/2023 04:32:45 AM [INFO] inverse_pra - 0
05/04/2023 04:32:45 AM [INFO] task - 3-way-classification
05/04/2023 04:32:45 AM [INFO] enable_kldiv - False
05/04/2023 04:32:45 AM [INFO] label_smoothing - 0.1
05/04/2023 04:32:45 AM [INFO] inference - False
05/04/2023 04:32:45 AM [INFO] online_mode - 0
05/04/2023 04:32:45 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 04:32:45 AM [INFO] arch - mtl
05/04/2023 04:32:45 AM [INFO] disable_mtl - 0
05/04/2023 04:32:45 AM [INFO] mtl_base_arch - vanilla
05/04/2023 04:32:45 AM [INFO] model_name - roberta-base
05/04/2023 04:32:45 AM [INFO] tf2_model_name - roberta-base
05/04/2023 04:32:45 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 04:32:45 AM [INFO] sentence_pooling - none
05/04/2023 04:32:45 AM [INFO] freeze_emb_layer - False
05/04/2023 04:32:45 AM [INFO] exp_count - 0
05/04/2023 04:32:45 AM [INFO] fp16 - 0
05/04/2023 04:32:45 AM [INFO] ------------------------------------------------------------
05/04/2023 04:32:45 AM [DEBUG] initiating training process...
05/04/2023 04:32:49 AM [DEBUG] ModelWrapper(
  (doc_encoder): TransformerModel(
    (tf2): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
  )
  (task_head): TexClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=3, bias=True)
  )
  (train_metric): Accuracy()
  (val_metric): Accuracy()
  (test_metric): Accuracy()
  (te_task_head): TexClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
  (te_train_metric): Accuracy()
  (te_val_metric): Accuracy()
)
05/04/2023 04:32:49 AM [INFO] Model has 125240069 trainable parameters
05/04/2023 04:32:53 AM [DEBUG] about to start training loop...
05/04/2023 04:32:56 AM [INFO] epoch : 0 - average_val_loss : 1.105716, overall_val_acc : 0.250000
05/04/2023 04:38:48 AM [INFO] epoch : 0 - average_val_loss : 1.259186, overall_val_acc : 0.500000
05/04/2023 04:38:48 AM [INFO] epoch : 0 - average_val_te_loss : 0.689442, overall_val_te_acc : 0.534296
05/04/2023 04:39:03 AM [INFO] epoch : 0 - average_train_loss : 1.339013, overall_train_acc : 0.460625
05/04/2023 04:39:03 AM [INFO] epoch : 0 - average_train_te_loss : 0.693274, overall_train_te_acc : 0.509843
05/04/2023 04:44:55 AM [INFO] epoch : 1 - average_val_loss : 1.161487, overall_val_acc : 0.597500
05/04/2023 04:44:55 AM [INFO] epoch : 1 - average_val_te_loss : 0.639768, overall_val_te_acc : 0.642599
05/04/2023 04:45:10 AM [INFO] epoch : 1 - average_train_loss : 1.197975, overall_train_acc : 0.601875
05/04/2023 04:45:10 AM [INFO] epoch : 1 - average_train_te_loss : 0.676376, overall_train_te_acc : 0.579751
05/04/2023 04:45:11 AM [DEBUG] training done.
05/04/2023 04:46:40 AM [INFO] featurizing the gcdc corpus: all for task: 3-way-classification with model architecture: mtl
05/04/2023 04:46:44 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 04:46:44 AM [DEBUG] working on test dataset 
05/04/2023 04:46:44 AM [DEBUG] <Done>
05/04/2023 04:46:44 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:46:44 AM [INFO] post-processing the dataset
05/04/2023 04:46:44 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 04:46:44 AM [INFO] featurizing the datasets..
05/04/2023 04:46:44 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 04:46:45 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:46:45 AM [INFO] label distribution in test dataset (total count: 800)
05/04/2023 04:46:45 AM [INFO] {2: 384, 1: 171, 0: 245}
05/04/2023 04:46:45 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:46:45 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 04:46:48 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 04:46:48 AM [DEBUG] working on train dataset 
05/04/2023 04:46:48 AM [DEBUG] <Done>
05/04/2023 04:46:48 AM [DEBUG] working on dev dataset 
05/04/2023 04:46:48 AM [DEBUG] <Done>
05/04/2023 04:46:48 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:46:49 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:46:49 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:46:49 AM [INFO] 
command line argument captured ..
05/04/2023 04:46:49 AM [INFO] ------------------------------------------------------------
05/04/2023 04:46:49 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 04:46:49 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints
05/04/2023 04:46:49 AM [INFO] gpus - 1
05/04/2023 04:46:49 AM [INFO] epochs - 10
05/04/2023 04:46:49 AM [INFO] batch_size - 2
05/04/2023 04:46:49 AM [INFO] learning_rate - 1e-06
05/04/2023 04:46:49 AM [INFO] clip_grad_norm - 0.0
05/04/2023 04:46:49 AM [INFO] weight_decay - 0.01
05/04/2023 04:46:49 AM [INFO] dropout_rate - 0.1
05/04/2023 04:46:49 AM [INFO] enable_scheduler - False
05/04/2023 04:46:49 AM [INFO] warmup_steps - 0.01
05/04/2023 04:46:49 AM [INFO] margin - 1.0
05/04/2023 04:46:49 AM [INFO] corpus - gcdc
05/04/2023 04:46:49 AM [INFO] sub_corpus - all
05/04/2023 04:46:49 AM [INFO] max_seq_len - 512
05/04/2023 04:46:49 AM [INFO] max_fact_count - 50
05/04/2023 04:46:49 AM [INFO] max_fact_seq_len - 50
05/04/2023 04:46:49 AM [INFO] permutation_count - 20
05/04/2023 04:46:49 AM [INFO] with_replacement - 1
05/04/2023 04:46:49 AM [INFO] train_dataset_count - None
05/04/2023 04:46:49 AM [INFO] val_dataset_count - None
05/04/2023 04:46:49 AM [INFO] test_dataset_count - 800
05/04/2023 04:46:49 AM [INFO] inverse_pra - 0
05/04/2023 04:46:49 AM [INFO] task - 3-way-classification
05/04/2023 04:46:49 AM [INFO] enable_kldiv - False
05/04/2023 04:46:49 AM [INFO] label_smoothing - 0.1
05/04/2023 04:46:49 AM [INFO] inference - True
05/04/2023 04:46:49 AM [INFO] online_mode - 0
05/04/2023 04:46:49 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 04:46:49 AM [INFO] arch - mtl
05/04/2023 04:46:49 AM [INFO] disable_mtl - 0
05/04/2023 04:46:49 AM [INFO] mtl_base_arch - vanilla
05/04/2023 04:46:49 AM [INFO] model_name - roberta-base
05/04/2023 04:46:49 AM [INFO] tf2_model_name - roberta-base
05/04/2023 04:46:49 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 04:46:49 AM [INFO] sentence_pooling - none
05/04/2023 04:46:49 AM [INFO] freeze_emb_layer - True
05/04/2023 04:46:49 AM [INFO] exp_count - 0
05/04/2023 04:46:49 AM [INFO] fp16 - 0
05/04/2023 04:46:49 AM [INFO] ------------------------------------------------------------
05/04/2023 04:46:49 AM [DEBUG] initiating inference process...
05/04/2023 04:46:54 AM [INFO] frozed embedding layer
05/04/2023 04:46:54 AM [DEBUG] loading the model from checkpoint : /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints
05/04/2023 04:48:26 AM [INFO] featurizing the gcdc corpus: all for task: 3-way-classification with model architecture: mtl
05/04/2023 04:48:29 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 04:48:29 AM [DEBUG] working on test dataset 
05/04/2023 04:48:29 AM [DEBUG] <Done>
05/04/2023 04:48:29 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:48:29 AM [INFO] post-processing the dataset
05/04/2023 04:48:29 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 04:48:29 AM [INFO] featurizing the datasets..
05/04/2023 04:48:29 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 04:48:30 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:48:30 AM [INFO] label distribution in test dataset (total count: 800)
05/04/2023 04:48:30 AM [INFO] {2: 384, 1: 171, 0: 245}
05/04/2023 04:48:30 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:48:30 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 04:48:33 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 04:48:33 AM [DEBUG] working on train dataset 
05/04/2023 04:48:33 AM [DEBUG] <Done>
05/04/2023 04:48:33 AM [DEBUG] working on dev dataset 
05/04/2023 04:48:33 AM [DEBUG] <Done>
05/04/2023 04:48:33 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:48:34 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:48:34 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:48:34 AM [INFO] 
command line argument captured ..
05/04/2023 04:48:34 AM [INFO] ------------------------------------------------------------
05/04/2023 04:48:34 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 04:48:34 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints
05/04/2023 04:48:34 AM [INFO] gpus - 1
05/04/2023 04:48:34 AM [INFO] epochs - 10
05/04/2023 04:48:34 AM [INFO] batch_size - 2
05/04/2023 04:48:34 AM [INFO] learning_rate - 1e-06
05/04/2023 04:48:34 AM [INFO] clip_grad_norm - 0.0
05/04/2023 04:48:34 AM [INFO] weight_decay - 0.01
05/04/2023 04:48:34 AM [INFO] dropout_rate - 0.1
05/04/2023 04:48:34 AM [INFO] enable_scheduler - False
05/04/2023 04:48:34 AM [INFO] warmup_steps - 0.01
05/04/2023 04:48:34 AM [INFO] margin - 1.0
05/04/2023 04:48:34 AM [INFO] corpus - gcdc
05/04/2023 04:48:34 AM [INFO] sub_corpus - all
05/04/2023 04:48:34 AM [INFO] max_seq_len - 512
05/04/2023 04:48:34 AM [INFO] max_fact_count - 50
05/04/2023 04:48:34 AM [INFO] max_fact_seq_len - 50
05/04/2023 04:48:34 AM [INFO] permutation_count - 20
05/04/2023 04:48:34 AM [INFO] with_replacement - 1
05/04/2023 04:48:34 AM [INFO] train_dataset_count - None
05/04/2023 04:48:34 AM [INFO] val_dataset_count - None
05/04/2023 04:48:34 AM [INFO] test_dataset_count - 800
05/04/2023 04:48:34 AM [INFO] inverse_pra - 0
05/04/2023 04:48:34 AM [INFO] task - 3-way-classification
05/04/2023 04:48:34 AM [INFO] enable_kldiv - False
05/04/2023 04:48:34 AM [INFO] label_smoothing - 0.1
05/04/2023 04:48:34 AM [INFO] inference - True
05/04/2023 04:48:34 AM [INFO] online_mode - 0
05/04/2023 04:48:34 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 04:48:34 AM [INFO] arch - mtl
05/04/2023 04:48:34 AM [INFO] disable_mtl - 0
05/04/2023 04:48:34 AM [INFO] mtl_base_arch - vanilla
05/04/2023 04:48:34 AM [INFO] model_name - roberta-base
05/04/2023 04:48:34 AM [INFO] tf2_model_name - roberta-base
05/04/2023 04:48:34 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 04:48:34 AM [INFO] sentence_pooling - none
05/04/2023 04:48:34 AM [INFO] freeze_emb_layer - True
05/04/2023 04:48:34 AM [INFO] exp_count - 0
05/04/2023 04:48:34 AM [INFO] fp16 - 0
05/04/2023 04:48:34 AM [INFO] ------------------------------------------------------------
05/04/2023 04:48:34 AM [DEBUG] initiating inference process...
05/04/2023 04:48:38 AM [INFO] frozed embedding layer
05/04/2023 04:48:38 AM [DEBUG] loading the model from checkpoint : /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints
05/04/2023 04:50:48 AM [INFO] featurizing the gcdc corpus: all for task: 3-way-classification with model architecture: mtl
05/04/2023 04:50:51 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 04:50:51 AM [DEBUG] working on test dataset 
05/04/2023 04:50:51 AM [DEBUG] <Done>
05/04/2023 04:50:51 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:50:51 AM [INFO] post-processing the dataset
05/04/2023 04:50:51 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 04:50:51 AM [INFO] featurizing the datasets..
05/04/2023 04:50:51 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 04:50:52 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:50:52 AM [INFO] label distribution in test dataset (total count: 800)
05/04/2023 04:50:52 AM [INFO] {2: 384, 1: 171, 0: 245}
05/04/2023 04:50:52 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:50:52 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 04:50:55 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 04:50:55 AM [DEBUG] working on train dataset 
05/04/2023 04:50:55 AM [DEBUG] <Done>
05/04/2023 04:50:55 AM [DEBUG] working on dev dataset 
05/04/2023 04:50:55 AM [DEBUG] <Done>
05/04/2023 04:50:55 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:50:56 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:50:56 AM [DEBUG] ------------------------------------------------------------
05/04/2023 04:50:56 AM [INFO] 
command line argument captured ..
05/04/2023 04:50:56 AM [INFO] ------------------------------------------------------------
05/04/2023 04:50:56 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 04:50:56 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints
05/04/2023 04:50:56 AM [INFO] gpus - 1
05/04/2023 04:50:56 AM [INFO] epochs - 10
05/04/2023 04:50:56 AM [INFO] batch_size - 2
05/04/2023 04:50:56 AM [INFO] learning_rate - 1e-06
05/04/2023 04:50:56 AM [INFO] clip_grad_norm - 0.0
05/04/2023 04:50:56 AM [INFO] weight_decay - 0.01
05/04/2023 04:50:56 AM [INFO] dropout_rate - 0.1
05/04/2023 04:50:56 AM [INFO] enable_scheduler - False
05/04/2023 04:50:56 AM [INFO] warmup_steps - 0.01
05/04/2023 04:50:56 AM [INFO] margin - 1.0
05/04/2023 04:50:56 AM [INFO] corpus - gcdc
05/04/2023 04:50:56 AM [INFO] sub_corpus - all
05/04/2023 04:50:56 AM [INFO] max_seq_len - 512
05/04/2023 04:50:56 AM [INFO] max_fact_count - 50
05/04/2023 04:50:56 AM [INFO] max_fact_seq_len - 50
05/04/2023 04:50:56 AM [INFO] permutation_count - 20
05/04/2023 04:50:56 AM [INFO] with_replacement - 1
05/04/2023 04:50:56 AM [INFO] train_dataset_count - None
05/04/2023 04:50:56 AM [INFO] val_dataset_count - None
05/04/2023 04:50:56 AM [INFO] test_dataset_count - 800
05/04/2023 04:50:56 AM [INFO] inverse_pra - 0
05/04/2023 04:50:56 AM [INFO] task - 3-way-classification
05/04/2023 04:50:56 AM [INFO] enable_kldiv - False
05/04/2023 04:50:56 AM [INFO] label_smoothing - 0.1
05/04/2023 04:50:56 AM [INFO] inference - True
05/04/2023 04:50:56 AM [INFO] online_mode - 0
05/04/2023 04:50:56 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 04:50:56 AM [INFO] arch - mtl
05/04/2023 04:50:56 AM [INFO] disable_mtl - 0
05/04/2023 04:50:56 AM [INFO] mtl_base_arch - vanilla
05/04/2023 04:50:56 AM [INFO] model_name - roberta-base
05/04/2023 04:50:56 AM [INFO] tf2_model_name - roberta-base
05/04/2023 04:50:56 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 04:50:56 AM [INFO] sentence_pooling - none
05/04/2023 04:50:56 AM [INFO] freeze_emb_layer - True
05/04/2023 04:50:56 AM [INFO] exp_count - 0
05/04/2023 04:50:56 AM [INFO] fp16 - 0
05/04/2023 04:50:56 AM [INFO] ------------------------------------------------------------
05/04/2023 04:50:56 AM [DEBUG] initiating inference process...
05/04/2023 04:51:00 AM [INFO] frozed embedding layer
05/04/2023 04:51:00 AM [DEBUG] loading the model from checkpoint : /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints
05/04/2023 05:02:28 AM [INFO] featurizing the gcdc corpus: all for task: 3-way-classification with model architecture: mtl
05/04/2023 05:02:31 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 05:02:31 AM [DEBUG] working on test dataset 
05/04/2023 05:02:31 AM [DEBUG] <Done>
05/04/2023 05:02:31 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:02:31 AM [INFO] post-processing the dataset
05/04/2023 05:02:31 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 05:02:31 AM [INFO] featurizing the datasets..
05/04/2023 05:02:31 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 05:02:32 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:02:32 AM [INFO] label distribution in test dataset (total count: 800)
05/04/2023 05:02:32 AM [INFO] {2: 384, 1: 171, 0: 245}
05/04/2023 05:02:32 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:02:32 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 05:02:35 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 05:02:35 AM [DEBUG] working on train dataset 
05/04/2023 05:02:35 AM [DEBUG] <Done>
05/04/2023 05:02:35 AM [DEBUG] working on dev dataset 
05/04/2023 05:02:35 AM [DEBUG] <Done>
05/04/2023 05:02:35 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:02:35 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:02:35 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:02:35 AM [INFO] 
command line argument captured ..
05/04/2023 05:02:35 AM [INFO] ------------------------------------------------------------
05/04/2023 05:02:35 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 05:02:35 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-mtl-vanilla-3-way-classification-roberta-base/epoch=1.ckpt
05/04/2023 05:02:35 AM [INFO] gpus - 1
05/04/2023 05:02:35 AM [INFO] epochs - 10
05/04/2023 05:02:35 AM [INFO] batch_size - 2
05/04/2023 05:02:35 AM [INFO] learning_rate - 1e-06
05/04/2023 05:02:35 AM [INFO] clip_grad_norm - 0.0
05/04/2023 05:02:35 AM [INFO] weight_decay - 0.01
05/04/2023 05:02:35 AM [INFO] dropout_rate - 0.1
05/04/2023 05:02:35 AM [INFO] enable_scheduler - False
05/04/2023 05:02:35 AM [INFO] warmup_steps - 0.01
05/04/2023 05:02:35 AM [INFO] margin - 1.0
05/04/2023 05:02:35 AM [INFO] corpus - gcdc
05/04/2023 05:02:35 AM [INFO] sub_corpus - all
05/04/2023 05:02:35 AM [INFO] max_seq_len - 512
05/04/2023 05:02:35 AM [INFO] max_fact_count - 50
05/04/2023 05:02:35 AM [INFO] max_fact_seq_len - 50
05/04/2023 05:02:35 AM [INFO] permutation_count - 20
05/04/2023 05:02:35 AM [INFO] with_replacement - 1
05/04/2023 05:02:35 AM [INFO] train_dataset_count - None
05/04/2023 05:02:35 AM [INFO] val_dataset_count - None
05/04/2023 05:02:35 AM [INFO] test_dataset_count - 800
05/04/2023 05:02:35 AM [INFO] inverse_pra - 0
05/04/2023 05:02:35 AM [INFO] task - 3-way-classification
05/04/2023 05:02:35 AM [INFO] enable_kldiv - False
05/04/2023 05:02:35 AM [INFO] label_smoothing - 0.1
05/04/2023 05:02:35 AM [INFO] inference - True
05/04/2023 05:02:35 AM [INFO] online_mode - 0
05/04/2023 05:02:35 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 05:02:35 AM [INFO] arch - mtl
05/04/2023 05:02:35 AM [INFO] disable_mtl - 0
05/04/2023 05:02:35 AM [INFO] mtl_base_arch - vanilla
05/04/2023 05:02:35 AM [INFO] model_name - roberta-base
05/04/2023 05:02:35 AM [INFO] tf2_model_name - roberta-base
05/04/2023 05:02:35 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 05:02:35 AM [INFO] sentence_pooling - none
05/04/2023 05:02:35 AM [INFO] freeze_emb_layer - True
05/04/2023 05:02:35 AM [INFO] exp_count - 0
05/04/2023 05:02:35 AM [INFO] fp16 - 0
05/04/2023 05:02:35 AM [INFO] ------------------------------------------------------------
05/04/2023 05:02:35 AM [DEBUG] initiating inference process...
05/04/2023 05:02:40 AM [INFO] frozed embedding layer
05/04/2023 05:02:40 AM [DEBUG] loading the model from checkpoint : /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-mtl-vanilla-3-way-classification-roberta-base/epoch=1.ckpt
05/04/2023 05:02:45 AM [INFO] frozed embedding layer
05/04/2023 05:02:46 AM [DEBUG] loaded model successfully !!!
05/04/2023 05:02:46 AM [INFO] testing on dataset : gcdc ( sub_dataset all ) on task : 3-way-classification
05/04/2023 05:03:03 AM [INFO] epoch : 0 - average_test_loss : 0.919560, overall_test_acc : 0.596250
05/04/2023 05:03:03 AM [DEBUG] testing done !!!
05/04/2023 05:09:00 AM [INFO] featurizing the gcdc corpus: all for task: 3-way-classification with model architecture: mtl
05/04/2023 05:09:03 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 05:09:03 AM [DEBUG] working on test dataset 
05/04/2023 05:09:03 AM [DEBUG] <Done>
05/04/2023 05:09:03 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:09:03 AM [INFO] post-processing the dataset
05/04/2023 05:09:03 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 05:09:03 AM [INFO] featurizing the datasets..
05/04/2023 05:09:03 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 05:09:04 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:09:04 AM [INFO] label distribution in test dataset (total count: 800)
05/04/2023 05:09:04 AM [INFO] {2: 384, 1: 171, 0: 245}
05/04/2023 05:09:04 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:09:04 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 05:09:06 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 05:09:06 AM [DEBUG] working on train dataset 
05/04/2023 05:09:06 AM [DEBUG] <Done>
05/04/2023 05:09:06 AM [DEBUG] working on dev dataset 
05/04/2023 05:09:06 AM [DEBUG] <Done>
05/04/2023 05:09:06 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:09:07 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:09:07 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:09:07 AM [INFO] 
command line argument captured ..
05/04/2023 05:09:07 AM [INFO] ------------------------------------------------------------
05/04/2023 05:09:07 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 05:09:07 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-mtl-vanilla-3-way-classification-roberta-base/epoch=1.ckpt
05/04/2023 05:09:07 AM [INFO] gpus - 1
05/04/2023 05:09:07 AM [INFO] epochs - 10
05/04/2023 05:09:07 AM [INFO] batch_size - 2
05/04/2023 05:09:07 AM [INFO] learning_rate - 1e-06
05/04/2023 05:09:07 AM [INFO] clip_grad_norm - 0.0
05/04/2023 05:09:07 AM [INFO] weight_decay - 0.01
05/04/2023 05:09:07 AM [INFO] dropout_rate - 0.1
05/04/2023 05:09:07 AM [INFO] enable_scheduler - False
05/04/2023 05:09:07 AM [INFO] warmup_steps - 0.01
05/04/2023 05:09:07 AM [INFO] margin - 1.0
05/04/2023 05:09:07 AM [INFO] corpus - gcdc
05/04/2023 05:09:07 AM [INFO] sub_corpus - all
05/04/2023 05:09:07 AM [INFO] max_seq_len - 512
05/04/2023 05:09:07 AM [INFO] max_fact_count - 50
05/04/2023 05:09:07 AM [INFO] max_fact_seq_len - 50
05/04/2023 05:09:07 AM [INFO] permutation_count - 20
05/04/2023 05:09:07 AM [INFO] with_replacement - 1
05/04/2023 05:09:07 AM [INFO] train_dataset_count - None
05/04/2023 05:09:07 AM [INFO] val_dataset_count - None
05/04/2023 05:09:07 AM [INFO] test_dataset_count - 800
05/04/2023 05:09:07 AM [INFO] inverse_pra - 0
05/04/2023 05:09:07 AM [INFO] task - 3-way-classification
05/04/2023 05:09:07 AM [INFO] enable_kldiv - False
05/04/2023 05:09:07 AM [INFO] label_smoothing - 0.1
05/04/2023 05:09:07 AM [INFO] inference - True
05/04/2023 05:09:07 AM [INFO] online_mode - 0
05/04/2023 05:09:07 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 05:09:07 AM [INFO] arch - mtl
05/04/2023 05:09:07 AM [INFO] disable_mtl - 0
05/04/2023 05:09:07 AM [INFO] mtl_base_arch - vanilla
05/04/2023 05:09:07 AM [INFO] model_name - roberta-base
05/04/2023 05:09:07 AM [INFO] tf2_model_name - roberta-base
05/04/2023 05:09:07 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 05:09:07 AM [INFO] sentence_pooling - none
05/04/2023 05:09:07 AM [INFO] freeze_emb_layer - True
05/04/2023 05:09:07 AM [INFO] exp_count - 0
05/04/2023 05:09:07 AM [INFO] fp16 - 0
05/04/2023 05:09:07 AM [INFO] ------------------------------------------------------------
05/04/2023 05:09:07 AM [DEBUG] initiating inference process...
05/04/2023 05:09:11 AM [INFO] frozed embedding layer
05/04/2023 05:09:11 AM [DEBUG] loading the model from checkpoint : /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-mtl-vanilla-3-way-classification-roberta-base/epoch=1.ckpt
05/04/2023 05:09:28 AM [INFO] frozed embedding layer
05/04/2023 05:09:28 AM [DEBUG] loaded model successfully !!!
05/04/2023 05:09:28 AM [INFO] testing on dataset : gcdc ( sub_dataset all ) on task : 3-way-classification
05/04/2023 05:09:50 AM [INFO] epoch : 0 - average_test_loss : 0.919560, overall_test_acc : 0.596250
05/04/2023 05:09:50 AM [DEBUG] testing done !!!
05/04/2023 05:35:14 AM [INFO] featurizing the gcdc corpus: All for task: 3-way-classification with model architecture: mtl
05/04/2023 05:35:18 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 05:35:18 AM [DEBUG] working on test dataset 
05/04/2023 05:35:18 AM [DEBUG] <Done>
05/04/2023 05:35:18 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:35:18 AM [INFO] post-processing the dataset
05/04/2023 05:35:18 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 05:35:18 AM [INFO] featurizing the datasets..
05/04/2023 05:35:18 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 05:35:18 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:35:18 AM [INFO] label distribution in test dataset (total count: 800)
05/04/2023 05:35:18 AM [INFO] {2: 384, 1: 171, 0: 245}
05/04/2023 05:35:18 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:35:18 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 05:35:21 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 05:35:21 AM [DEBUG] working on train dataset 
05/04/2023 05:35:21 AM [DEBUG] <Done>
05/04/2023 05:35:21 AM [DEBUG] working on dev dataset 
05/04/2023 05:35:21 AM [DEBUG] <Done>
05/04/2023 05:35:21 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:35:22 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:35:22 AM [DEBUG] ------------------------------------------------------------
05/04/2023 05:35:22 AM [INFO] 
command line argument captured ..
05/04/2023 05:35:22 AM [INFO] ------------------------------------------------------------
05/04/2023 05:35:22 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 05:35:22 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-mtl-vanilla-3-way-classification-roberta-base/epoch=1.ckpt
05/04/2023 05:35:22 AM [INFO] gpus - 1
05/04/2023 05:35:22 AM [INFO] epochs - 10
05/04/2023 05:35:22 AM [INFO] batch_size - 2
05/04/2023 05:35:22 AM [INFO] learning_rate - 1e-06
05/04/2023 05:35:22 AM [INFO] clip_grad_norm - 0.0
05/04/2023 05:35:22 AM [INFO] weight_decay - 0.01
05/04/2023 05:35:22 AM [INFO] dropout_rate - 0.1
05/04/2023 05:35:22 AM [INFO] enable_scheduler - False
05/04/2023 05:35:22 AM [INFO] warmup_steps - 0.01
05/04/2023 05:35:22 AM [INFO] margin - 1.0
05/04/2023 05:35:22 AM [INFO] corpus - gcdc
05/04/2023 05:35:22 AM [INFO] sub_corpus - All
05/04/2023 05:35:22 AM [INFO] max_seq_len - 512
05/04/2023 05:35:22 AM [INFO] max_fact_count - 50
05/04/2023 05:35:22 AM [INFO] max_fact_seq_len - 50
05/04/2023 05:35:22 AM [INFO] permutation_count - 20
05/04/2023 05:35:22 AM [INFO] with_replacement - 1
05/04/2023 05:35:22 AM [INFO] train_dataset_count - None
05/04/2023 05:35:22 AM [INFO] val_dataset_count - None
05/04/2023 05:35:22 AM [INFO] test_dataset_count - 800
05/04/2023 05:35:22 AM [INFO] inverse_pra - 0
05/04/2023 05:35:22 AM [INFO] task - 3-way-classification
05/04/2023 05:35:22 AM [INFO] enable_kldiv - False
05/04/2023 05:35:22 AM [INFO] label_smoothing - 0.1
05/04/2023 05:35:22 AM [INFO] inference - True
05/04/2023 05:35:22 AM [INFO] online_mode - 0
05/04/2023 05:35:22 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 05:35:22 AM [INFO] arch - mtl
05/04/2023 05:35:22 AM [INFO] disable_mtl - 0
05/04/2023 05:35:22 AM [INFO] mtl_base_arch - vanilla
05/04/2023 05:35:22 AM [INFO] model_name - roberta-base
05/04/2023 05:35:22 AM [INFO] tf2_model_name - roberta-base
05/04/2023 05:35:22 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 05:35:22 AM [INFO] sentence_pooling - none
05/04/2023 05:35:22 AM [INFO] freeze_emb_layer - True
05/04/2023 05:35:22 AM [INFO] exp_count - 0
05/04/2023 05:35:22 AM [INFO] fp16 - 0
05/04/2023 05:35:22 AM [INFO] ------------------------------------------------------------
05/04/2023 05:35:22 AM [DEBUG] initiating inference process...
05/04/2023 05:35:26 AM [INFO] frozed embedding layer
05/04/2023 05:35:26 AM [DEBUG] loading the model from checkpoint : /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-mtl-vanilla-3-way-classification-roberta-base/epoch=1.ckpt
05/04/2023 05:35:31 AM [INFO] frozed embedding layer
05/04/2023 05:35:32 AM [DEBUG] loaded model successfully !!!
05/04/2023 05:35:32 AM [INFO] testing on dataset : gcdc ( sub_dataset All ) on task : 3-way-classification
05/04/2023 05:35:54 AM [INFO] epoch : 0 - average_test_loss : 0.919560, overall_test_acc : 0.596250
05/04/2023 05:35:54 AM [DEBUG] testing done !!!
05/04/2023 08:08:46 AM [INFO] featurizing the gcdc corpus: All for task: 3-way-classification with model architecture: mtl
05/04/2023 08:08:50 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 08:08:50 AM [DEBUG] working on test dataset 
05/04/2023 08:08:50 AM [DEBUG] <Done>
05/04/2023 08:08:50 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:08:50 AM [INFO] post-processing the dataset
05/04/2023 08:08:50 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 08:08:50 AM [INFO] featurizing the datasets..
05/04/2023 08:08:50 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 08:08:51 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:08:51 AM [INFO] label distribution in test dataset (total count: 800)
05/04/2023 08:08:51 AM [INFO] {2: 384, 1: 171, 0: 245}
05/04/2023 08:08:51 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:08:51 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 08:08:54 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 08:08:54 AM [DEBUG] working on train dataset 
05/04/2023 08:08:54 AM [DEBUG] <Done>
05/04/2023 08:08:54 AM [DEBUG] working on dev dataset 
05/04/2023 08:08:54 AM [DEBUG] <Done>
05/04/2023 08:08:54 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:08:55 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:08:55 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:08:55 AM [INFO] 
command line argument captured ..
05/04/2023 08:08:55 AM [INFO] ------------------------------------------------------------
05/04/2023 08:08:55 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 08:08:55 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-mtl-vanilla-3-way-classification-roberta-base/epoch=0.ckpt
05/04/2023 08:08:55 AM [INFO] gpus - 1
05/04/2023 08:08:55 AM [INFO] epochs - 10
05/04/2023 08:08:55 AM [INFO] batch_size - 2
05/04/2023 08:08:55 AM [INFO] learning_rate - 1e-06
05/04/2023 08:08:55 AM [INFO] clip_grad_norm - 0.0
05/04/2023 08:08:55 AM [INFO] weight_decay - 0.01
05/04/2023 08:08:55 AM [INFO] dropout_rate - 0.1
05/04/2023 08:08:55 AM [INFO] enable_scheduler - False
05/04/2023 08:08:55 AM [INFO] warmup_steps - 0.01
05/04/2023 08:08:55 AM [INFO] margin - 1.0
05/04/2023 08:08:55 AM [INFO] corpus - gcdc
05/04/2023 08:08:55 AM [INFO] sub_corpus - All
05/04/2023 08:08:55 AM [INFO] max_seq_len - 512
05/04/2023 08:08:55 AM [INFO] max_fact_count - 50
05/04/2023 08:08:55 AM [INFO] max_fact_seq_len - 50
05/04/2023 08:08:55 AM [INFO] permutation_count - 20
05/04/2023 08:08:55 AM [INFO] with_replacement - 1
05/04/2023 08:08:55 AM [INFO] train_dataset_count - None
05/04/2023 08:08:55 AM [INFO] val_dataset_count - None
05/04/2023 08:08:55 AM [INFO] test_dataset_count - 800
05/04/2023 08:08:55 AM [INFO] inverse_pra - 0
05/04/2023 08:08:55 AM [INFO] task - 3-way-classification
05/04/2023 08:08:55 AM [INFO] enable_kldiv - False
05/04/2023 08:08:55 AM [INFO] label_smoothing - 0.1
05/04/2023 08:08:55 AM [INFO] inference - True
05/04/2023 08:08:55 AM [INFO] online_mode - 0
05/04/2023 08:08:55 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 08:08:55 AM [INFO] arch - mtl
05/04/2023 08:08:55 AM [INFO] disable_mtl - 0
05/04/2023 08:08:55 AM [INFO] mtl_base_arch - vanilla
05/04/2023 08:08:55 AM [INFO] model_name - roberta-base
05/04/2023 08:08:55 AM [INFO] tf2_model_name - roberta-base
05/04/2023 08:08:55 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 08:08:55 AM [INFO] sentence_pooling - none
05/04/2023 08:08:55 AM [INFO] freeze_emb_layer - True
05/04/2023 08:08:55 AM [INFO] exp_count - 0
05/04/2023 08:08:55 AM [INFO] fp16 - 0
05/04/2023 08:08:55 AM [INFO] ------------------------------------------------------------
05/04/2023 08:08:55 AM [DEBUG] initiating inference process...
05/04/2023 08:09:00 AM [INFO] frozed embedding layer
05/04/2023 08:09:00 AM [DEBUG] loading the model from checkpoint : /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-mtl-vanilla-3-way-classification-roberta-base/epoch=0.ckpt
05/04/2023 08:10:38 AM [INFO] featurizing the gcdc corpus: All for task: 3-way-classification with model architecture: mtl
05/04/2023 08:10:41 AM [DEBUG] <<LOADING>> GCDC dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/GCDC
05/04/2023 08:10:41 AM [DEBUG] working on test dataset 
05/04/2023 08:10:42 AM [DEBUG] <Done>
05/04/2023 08:10:42 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:10:42 AM [INFO] post-processing the dataset
05/04/2023 08:10:42 AM [INFO] data preprocessed: 800, sentences preprocessed: 800
05/04/2023 08:10:42 AM [INFO] featurizing the datasets..
05/04/2023 08:10:42 AM [DEBUG] 800 data instance processed. max sent_seq_length: 512
05/04/2023 08:10:42 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:10:42 AM [INFO] label distribution in test dataset (total count: 800)
05/04/2023 08:10:42 AM [INFO] {2: 384, 1: 171, 0: 245}
05/04/2023 08:10:42 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:10:42 AM [INFO] featurizing the textual entailment corpus for task: 3-way-classification with model architecture: mtl
05/04/2023 08:10:46 AM [DEBUG] <<LOADING>> RTE dataset from directory: /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data/RTE
05/04/2023 08:10:46 AM [DEBUG] working on train dataset 
05/04/2023 08:10:46 AM [DEBUG] <Done>
05/04/2023 08:10:46 AM [DEBUG] working on dev dataset 
05/04/2023 08:10:46 AM [DEBUG] <Done>
05/04/2023 08:10:46 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:10:47 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:10:47 AM [DEBUG] ------------------------------------------------------------
05/04/2023 08:10:47 AM [INFO] 
command line argument captured ..
05/04/2023 08:10:47 AM [INFO] ------------------------------------------------------------
05/04/2023 08:10:47 AM [INFO] processed_dataset_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/processed_data
05/04/2023 08:10:47 AM [INFO] checkpoint_path - /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-fact-aware-3-way-classification-roberta-base/epoch=0.ckpt
05/04/2023 08:10:47 AM [INFO] gpus - 1
05/04/2023 08:10:47 AM [INFO] epochs - 10
05/04/2023 08:10:47 AM [INFO] batch_size - 2
05/04/2023 08:10:47 AM [INFO] learning_rate - 1e-06
05/04/2023 08:10:47 AM [INFO] clip_grad_norm - 0.0
05/04/2023 08:10:47 AM [INFO] weight_decay - 0.01
05/04/2023 08:10:47 AM [INFO] dropout_rate - 0.1
05/04/2023 08:10:47 AM [INFO] enable_scheduler - False
05/04/2023 08:10:47 AM [INFO] warmup_steps - 0.01
05/04/2023 08:10:47 AM [INFO] margin - 1.0
05/04/2023 08:10:47 AM [INFO] corpus - gcdc
05/04/2023 08:10:47 AM [INFO] sub_corpus - All
05/04/2023 08:10:47 AM [INFO] max_seq_len - 512
05/04/2023 08:10:47 AM [INFO] max_fact_count - 50
05/04/2023 08:10:47 AM [INFO] max_fact_seq_len - 50
05/04/2023 08:10:47 AM [INFO] permutation_count - 20
05/04/2023 08:10:47 AM [INFO] with_replacement - 1
05/04/2023 08:10:47 AM [INFO] train_dataset_count - None
05/04/2023 08:10:47 AM [INFO] val_dataset_count - None
05/04/2023 08:10:47 AM [INFO] test_dataset_count - 800
05/04/2023 08:10:47 AM [INFO] inverse_pra - 0
05/04/2023 08:10:47 AM [INFO] task - 3-way-classification
05/04/2023 08:10:47 AM [INFO] enable_kldiv - False
05/04/2023 08:10:47 AM [INFO] label_smoothing - 0.1
05/04/2023 08:10:47 AM [INFO] inference - True
05/04/2023 08:10:47 AM [INFO] online_mode - 0
05/04/2023 08:10:47 AM [INFO] logger_exp_name - gcdc-All-mtl-vanilla-3-way-classification-roberta-base
05/04/2023 08:10:47 AM [INFO] arch - mtl
05/04/2023 08:10:47 AM [INFO] disable_mtl - 0
05/04/2023 08:10:47 AM [INFO] mtl_base_arch - vanilla
05/04/2023 08:10:47 AM [INFO] model_name - roberta-base
05/04/2023 08:10:47 AM [INFO] tf2_model_name - roberta-base
05/04/2023 08:10:47 AM [INFO] use_pretrained_tf2 - 0
05/04/2023 08:10:47 AM [INFO] sentence_pooling - none
05/04/2023 08:10:47 AM [INFO] freeze_emb_layer - True
05/04/2023 08:10:47 AM [INFO] exp_count - 0
05/04/2023 08:10:47 AM [INFO] fp16 - 0
05/04/2023 08:10:47 AM [INFO] ------------------------------------------------------------
05/04/2023 08:10:47 AM [DEBUG] initiating inference process...
05/04/2023 08:10:51 AM [INFO] frozed embedding layer
05/04/2023 08:10:51 AM [DEBUG] loading the model from checkpoint : /home2/devesh.marwah/Transformer-Models-for-Text-Coherence-Assessment/lightning_checkpoints/gcdc-All-fact-aware-3-way-classification-roberta-base/epoch=0.ckpt
05/04/2023 08:11:01 AM [INFO] frozed embedding layer
